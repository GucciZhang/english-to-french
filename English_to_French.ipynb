{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English to French.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1TPCI3vfhPNQy_gcduZoFnWlv0Bp5q4dh",
      "authorship_tag": "ABX9TyMMBQX4spZpLpQYHHio9XG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GucciZhang/english-to-french/blob/main/English_to_French.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc8qxSRr0mz1",
        "outputId": "876e2619-545e-4a84-f8d2-b74e0f1c872b"
      },
      "source": [
        "!git clone https://github.com/GucciZhang/english-to-french.git\n",
        "%cd english-to-french/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'english-to-french'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 2 (delta 0), pack-reused 10\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n",
            "/content/english-to-french/english-to-french\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Installing additional required modules \n",
        "'''\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ],
      "metadata": {
        "id": "59T0_diOBQUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8104f283-ea00-4a78-c673-043495d9b872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 14.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "Collecting fr_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.2.5/fr_core_news_sm-2.2.5.tar.gz (14.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.7 MB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from fr_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->fr_core_news_sm==2.2.5) (1.24.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('fr_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a_a4QoRjC3x"
      },
      "source": [
        "'''\n",
        "  Setup Pytorch and other imports\n",
        "'''\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext.datasets import IWSLT2016\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import en_core_web_sm\n",
        "import fr_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "from collections import Counter\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "  Data processing\n",
        "'''\n",
        "\n",
        "# Tokenizers\n",
        "spacy_en = en_core_web_sm.load()\n",
        "spacy_fr = fr_core_news_sm.load()\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_fr(text):\n",
        "  return [token.text for token in spacy_fr.tokenizer(text)]\n",
        "\n",
        "train_iter, valid_iter, test_iter = IWSLT2016(language_pair=('en', 'fr'))\n",
        "\n",
        "def tokenize_data(data_iter):\n",
        "  # Tokenize source and target sentences\n",
        "  data = []\n",
        "  for en, fr in data_iter:\n",
        "    en = en.lower().strip()\n",
        "    fr = fr.lower().strip()\n",
        "    data.append({'src': tokenize_en(en), 'trg': tokenize_fr(fr)})\n",
        "\n",
        "  return data\n",
        "\n",
        "# Data splits\n",
        "train_data_raw = tokenize_data(train_iter)\n",
        "valid_data_raw = tokenize_data(valid_iter)\n",
        "test_data_raw = tokenize_data(test_iter)"
      ],
      "metadata": {
        "id": "1hfMzoMDMXMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data_raw[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyxYIwYUVFAb",
        "outputId": "c8fbae1d-0992-41b5-f4c4-8474ed08c9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'src': ['david', 'gallo', ':', 'this', 'is', 'bill', 'lange', '.', 'i', \"'m\", 'dave', 'gallo', '.'], 'trg': ['david', 'gallo', ':', 'voici', 'bill', 'lange', '.', 'je', 'suis', 'dave', 'gallo', '.']}, {'src': ['and', 'we', \"'re\", 'going', 'to', 'tell', 'you', 'some', 'stories', 'from', 'the', 'sea', 'here', 'in', 'video', '.'], 'trg': ['nous', 'allons', 'vous', 'raconter', 'quelques', 'histoires', 'de', 'la', 'mer', 'en', 'vidéo', '.']}, {'src': ['we', \"'ve\", 'got', 'some', 'of', 'the', 'most', 'incredible', 'video', 'of', 'titanic', 'that', \"'s\", 'ever', 'been', 'seen', ',', 'and', 'we', \"'re\", 'not', 'going', 'to', 'show', 'you', 'any', 'of', 'it', '.'], 'trg': ['nous', 'avons', 'des', 'vidéos', 'du', 'titanic', 'parmi', 'les', 'plus', 'spectaculaires', 'jamais', 'vues', '.', 'et', 'nous', \"n'\", 'allons', 'pas', 'vous', 'en', 'montrer', 'une', 'image', '.']}, {'src': ['the', 'truth', 'of', 'the', 'matter', 'is', 'that', 'the', 'titanic', '--', 'even', 'though', 'it', \"'s\", 'breaking', 'all', 'sorts', 'of', 'box', 'office', 'records', '--', 'it', \"'s\", 'not', 'the', 'most', 'exciting', 'story', 'from', 'the', 'sea', '.'], 'trg': ['la', 'vérité', 'est', 'que', 'le', 'titanic', '--', 'même', \"s'\", 'il', 'continue', 'de', 'battre', 'toutes', 'les', 'records', 'de', 'recettes', '--', \"n'\", 'est', 'pas', \"l'\", 'histoire', 'la', 'plus', 'passionnante', '.']}, {'src': ['and', 'the', 'problem', ',', 'i', 'think', ',', 'is', 'that', 'we', 'take', 'the', 'ocean', 'for', 'granted', '.'], 'trg': ['le', 'problème', ',', 'je', 'crois', ',', 'est', \"qu'\", 'on', 'tient', \"l'\", 'océan', 'pour', 'acquis', '.']}, {'src': ['when', 'you', 'think', 'about', 'it', ',', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet', '.'], 'trg': ['quand', 'vous', 'y', 'pensez', ',', 'les', 'océans', 'représentent', '75', '%', 'de', 'la', 'planète', '.']}, {'src': ['most', 'of', 'the', 'planet', 'is', 'ocean', 'water', '.'], 'trg': ['la', 'plus', 'grande', 'partie', 'de', 'la', 'planète', 'est', \"d'\", 'eau', '.']}, {'src': ['the', 'average', 'depth', 'is', 'about', 'two', 'miles', '.'], 'trg': ['la', 'profondeur', 'moyenne', 'est', 'environ', '3,2', 'km', '.']}, {'src': ['part', 'of', 'the', 'problem', ',', 'i', 'think', ',', 'is', 'we', 'stand', 'at', 'the', 'beach', ',', 'or', 'we', 'see', 'images', 'like', 'this', 'of', 'the', 'ocean', ',', 'and', 'you', 'look', 'out', 'at', 'this', 'great', 'big', 'blue', 'expanse', ',', 'and', 'it', \"'s\", 'shimmering', 'and', 'it', \"'s\", 'moving', 'and', 'there', \"'s\", 'waves', 'and', 'there', \"'s\", 'surf', 'and', 'there', \"'s\", 'tides', ',', 'but', 'you', 'have', 'no', 'idea', 'for', 'what', 'lies', 'in', 'there', '.'], 'trg': ['une', 'partie', 'du', 'problème', ',', 'je', 'pense', ',', 'est', \"qu'\", 'en', 'étant', 'sur', 'la', 'plage', 'ou', 'en', 'regardant', 'des', 'images', 'de', \"l'\", 'océan', ',', 'comme', 'celles', '-', 'ci', ',', 'on', 'voit', 'cette', 'grande', 'étendue', 'bleue', ',', 'chatoyante', ',', 'ça', 'bouge', ',', 'il', 'y', 'a', 'des', 'vagues', ',', 'il', 'y', 'a', 'du', 'surf', 'et', 'il', 'y', 'a', 'des', 'marées', ',', 'mais', 'vous', \"n'\", 'avez', 'aucune', 'idée', 'de', 'ce', 'qui', \"s'\", 'y', 'cache', '.']}, {'src': ['and', 'in', 'the', 'oceans', ',', 'there', 'are', 'the', 'longest', 'mountain', 'ranges', 'on', 'the', 'planet', '.'], 'trg': ['il', 'y', 'existe', 'les', 'chaînes', 'de', 'montagnes', 'les', 'plus', 'longues', 'de', 'la', 'planète', '.']}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a vocabulary for English and French, which is a set of all the words that show up in the training data. Each word will also be assigned a numeric value which will be used later to encode the words (one-hot encoding) as tensors. \n",
        "\n",
        "Note that we only consider the words in the training data. Words not including in the training data (and hence not learned by the neural network) will be mapped to the '<unk>' token which stands for unknown."
      ],
      "metadata": {
        "id": "ECGoVTO3tb3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        " Building the vocabulary\n",
        "'''\n",
        "\n",
        "def build_vocab(data):\n",
        "  \"\"\"\n",
        "    Generates the vocabulary of provided data (list of lists of tokens)\n",
        "    Note <unk> for unknown tokens, <pad> for padding, <bos> is beginning of strings, <eos> is end of string\n",
        "  \"\"\"\n",
        "  return build_vocab_from_iterator(data, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "\n",
        "en_vocab = build_vocab((pair['src'] for pair in train_data_raw))\n",
        "en_vocab.set_default_index(en_vocab['<unk>'])\n",
        "fr_vocab = build_vocab((pair['trg'] for pair in train_data_raw))\n",
        "fr_vocab.set_default_index(fr_vocab['<unk>'])"
      ],
      "metadata": {
        "id": "S2Z6yhMmYDqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unique tokens in source (en) vocabulary: {len(en_vocab)}\")\n",
        "print(f\"Unique tokens in target (fr) vocabulary: {len(fr_vocab)}\")\n",
        "\n",
        "# print(en_vocab.get_itos())\n",
        "# print(fr_vocab.get_itos())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfTb4ONnYQtr",
        "outputId": "61da12e7-58d3-42de-a12c-ade4c5997a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in source (en) vocabulary: 53678\n",
            "Unique tokens in target (fr) vocabulary: 74590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_data(raw_data):\n",
        "  # Encoding the data as torch tensors\n",
        "  data = []\n",
        "  for pair in raw_data:\n",
        "    src = pair['src']\n",
        "    trg = pair['trg']\n",
        "    src_tensor = torch.tensor([en_vocab[token] for token in src], dtype=torch.long)\n",
        "    trg_tensor = torch.tensor([fr_vocab[token] for token in trg], dtype=torch.long)\n",
        "    data.append((src_tensor, trg_tensor))\n",
        "  return data\n",
        "\n",
        "train_data = encode_data(train_data_raw)\n",
        "valid_data = encode_data(valid_data_raw)\n",
        "test_data = encode_data(test_data_raw)"
      ],
      "metadata": {
        "id": "v7Kh8DrymCwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "# The en and fr values for the special tokens are the same\n",
        "PAD_IDX = en_vocab['<pad>']\n",
        "BOS_IDX = en_vocab['<bos>']\n",
        "EOS_IDX = en_vocab['<eos>']\n",
        "\n",
        "def generate_batch(batch):\n",
        "  en_batch, fr_batch = [], []\n",
        "  for en_tensor, fr_tensor in batch:\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    fr_batch.append(torch.cat([torch.tensor([BOS_IDX]), fr_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  en_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  fr_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return en_batch, fr_batch\n",
        "\n",
        "train_data_iter = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "valid_data_iter = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
        "test_data_iter = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n"
      ],
      "metadata": {
        "id": "e3TvEGPnnjFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**\n",
        "\n",
        "Input vectors are one-hot encodings (very sparse, extra dimensions), will be converted to embeddings (denser, more efficient).\n",
        "\n",
        "We will have an initial dropout layer from the embedded inputs into our LSTM.\n",
        "The LSTM also takes a dropout argument - note this is dropout between layers of a multi-layer LSTM, not between recurrent iterations.\n",
        "\n",
        "The forward defines how our encoder handles input data. It first converts from one-hot vectors to embedded vectors using the embedding layer. Then, it applies dropout. The embeddings are then passed into the LSTM. Note we are passing in a sequence, but the Torch RNNs automatically handle the recurrent iterations for us."
      ],
      "metadata": {
        "id": "1dET2ZdlvgVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "\n",
        "    # LSTM RNN\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout)\n",
        "\n",
        "    # Initial dropout layer\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, seq):\n",
        "\n",
        "    # src: [seq length, batch size]\n",
        "\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    # embedded: [ seq length, batch size, embedding_dim ]\n",
        "\n",
        "    output, hidden, cell = self.lstm(embedded)\n",
        "\n",
        "    # output: [ seq length, batch size, hidden dim ]\n",
        "    # hidden: [ num layers, batch size, hidden dim ]\n",
        "    # cell: [ num layers, batch size, hidden dim]\n",
        "\n",
        "    return hidden, cell\n"
      ],
      "metadata": {
        "id": "7bJz1JATrH7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**\n",
        "\n",
        "The output dimension, analagously to the encoder, here is the size of the vocabulary for the target (French)."
      ],
      "metadata": {
        "id": "DzQJpk5YEANp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_dim = output_dim\n",
        "    self.hidden_dim = embedding_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
        "\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.predict = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "   \n",
        "  def forward(self, input, hidden, cell):\n",
        "    input = input.unsqueeze(0)\n",
        "\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "\n",
        "    output, hidden, cell = self.lstm(embedded, (hidden, cell))\n",
        "\n",
        "    prediction = self.predict(output.squeeze(0))\n",
        "\n",
        "    return prediction, hidden, cell\n"
      ],
      "metadata": {
        "id": "Ena_bUqaD_9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Seq2Seq**"
      ],
      "metadata": {
        "id": "qdOETibTHmsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, src, trg, teacher_forcing = 0.5):\n",
        "\n",
        "    # trg: [ trg_length, batch_size ]\n",
        "\n",
        "    trg_length, batch_size = trg.shape\n",
        "    trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "    # tensor storing decoder outputs\n",
        "    outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "    # initial hidden, cell states of decoder are the final hidden, cell states of the encoder\n",
        "    hidden, cell = self.encoder(src)\n",
        "\n",
        "    input = trg[0,:]\n",
        "\n",
        "    for t in range(1, trg_length):\n",
        "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "      # store prediction\n",
        "      outputs[t] = output\n",
        "\n",
        "      # decide if we will use teacher-forcing\n",
        "      teacher_force = random.random() < teacher_forcing\n",
        "\n",
        "      if teacher_force:\n",
        "        input = trg[t,:]\n",
        "      else:\n",
        "        prediction = output.argmax(1)\n",
        "        input = prediction\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zi46J8nNHpxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "UmIvB0ncLIjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(en_vocab)\n",
        "OUTPUT_DIM = len(fr_vocab)\n",
        "ENCODER_EMBEDDING_DIM = 256\n",
        "DECODER_EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "NUM_LAYERS = 4\n",
        "ENCODER_DROPOUT = 0.5\n",
        "DECODER_DROPOUT = 0.5\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENCODER_EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, ENCODER_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DECODER_EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DECODER_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device)\n",
        "\n",
        "def init_weights(model: nn.Module):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.uniform_(param.data, -0.05, 0.05)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BStyiAmXLKlg",
        "outputId": "404462d1-c041-4c91-b307-6da722d736f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(53678, 256)\n",
              "    (lstm): LSTM(256, 512, num_layers=4, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(74590, 256)\n",
              "    (lstm): LSTM(256, 512, num_layers=4, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (predict): Linear(in_features=512, out_features=74590, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlwP8dvIMHip",
        "outputId": "6ba766c9-92a2-455c-a93c-3c80e7066f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 86,862,686 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qvgaRgtzOINC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}